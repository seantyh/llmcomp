{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d29ca1-095a-4615-b372-b8725424efb9",
   "metadata": {},
   "source": [
    "## Data dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e7bdb7b-e7e2-411f-ae40-7771a3a5f026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5236da3d32f4a5a1afe42eedc40e1a0fe7b87edc  ../data/pwn_semrel_dataset.json\n",
      "775a757926fc27ca1426f7c3ac9cb03243e61a75  ../data/pythia-pwn-semrel-symtable.json\n"
     ]
    }
   ],
   "source": [
    "!sha1sum ../data/pwn_semrel_dataset.json # 14.10\n",
    "!sha1sum ../data/pythia-pwn-semrel-symtable.json # 22.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc11f4c1-729a-47aa-9d2d-beae58da1c95",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f800703-a480-42b2-8210-060cdc3293cf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "repo_name = \"EleutherAI/pythia-6.9b\"\n",
    "rev_idx = 0\n",
    "print(f\"Repo: {repo_name}, rev_idx: {rev_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7a1746e-2ecf-4e85-b838-5cf0c530853d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EleutherAI/pythia-6.9b\n"
     ]
    }
   ],
   "source": [
    "model_name = repo_name.split(\"/\")[1]\n",
    "print(repo_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5df5986-3cac-4936-ba13-75a215651065",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3445393f-a4a9-4981-a585-977b4201a78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"BNB_CUDA_VERSION\"] = \"115\"\n",
    "if \"../src/llm_compressor/src\" not in sys.path:\n",
    "  sys.path.append(\"../src/llm_compressor/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5df8c8f9-4a47-4978-bd81-0a07152e3daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seantyh/miniconda3/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:106: UserWarning: \n",
      "\n",
      "================================================================================\n",
      "WARNING: Manual override via BNB_CUDA_VERSION env variable detected!\n",
      "BNB_CUDA_VERSION=XXX can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "Loading CUDA version: BNB_CUDA_VERSION=115\n",
      "================================================================================\n",
      "\n",
      "\n",
      "  warn((f'\\n\\n{\"=\"*80}\\n'\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForCausalLM, GPTNeoXForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import list_repo_refs \n",
    "\n",
    "from llm_compressor import AECompressorLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c7f02c0-22c5-4d74-9bab-85ba50bd3ca8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = json.loads(Path(\"../data/pwn_semrel_dataset.json\").read_text())\n",
    "symtable = json.loads(Path(\"../data/pythia-pwn-semrel-symtable.json\").read_text())\n",
    "rev_symtable = {v: int(k) for k, v in symtable.items()}\n",
    "symlist = list(symtable.values())\n",
    "n_symbol = len(symlist)\n",
    "symbol_bits = np.log2(n_symbol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9168c03d-27b4-43b2-bbfa-1c063065252f",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc0a651",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65889625-b934-4072-93a8-4fe1f9936e88",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003198385238647461,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 7,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a514404f382e441daa8a59f9d3863951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reformat the rev_list in reverse order\n",
    "rev_list = [\"main\", \"step104000\", \"step65000\", \"step26000\", \"step13000\", \"step7000\", \"step1\"]\n",
    "rev_name = rev_list[rev_idx]\n",
    "cache_dir = Path(\"/home/seantyh/hdd/hf_cache\") / model_name / rev_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60d6d7f4-619e-4121-96f3-6bfbe09a6096",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003532886505126953,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c09f1c3f647146eda210ca4f80a8506a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(repo_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  repo_name,\n",
    "  revision=rev_name,\n",
    "  cache_dir=cache_dir,\n",
    "  load_in_8bit=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0d39a9a-827d-4e3f-9943-832f3ba25f66",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "VarLoc = namedtuple(\"VarLoc\", (\"name\", \"value\", \"start\", \"end\"))\n",
    "\n",
    "def make_locmap(inst):\n",
    "  return {\n",
    "    x[0]: VarLoc(*x)\n",
    "    for x in inst[\"var_loc\"]\n",
    "  }\n",
    "\n",
    "def find_token_locs(batch, char_locs):\n",
    "  return [\n",
    "    batch.char_to_token(0, x)\n",
    "    for x in char_locs\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1db0a0-2253-4ab9-ae88-71c25c6774fd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "752a149e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0032300949096679688,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2898,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9adac4b7f6e41e3975404f33137c1b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2898 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "proc_data = []\n",
    "for inst_x in tqdm(data):\n",
    "    batch = tokenizer(inst_x[\"prompt\"], \n",
    "                    return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        out = model(**batch, output_hidden_states=True)\n",
    "\n",
    "    var_loc_x = make_locmap(inst_x)\n",
    "\n",
    "    a_loc = var_loc_x[\"a_lemma\"]\n",
    "    b_loc = var_loc_x[\"b_lemma\"]\n",
    "    a_toklocs = find_token_locs(batch, (a_loc.start, a_loc.end-1))\n",
    "    b_toklocs = find_token_locs(batch, (b_loc.start, b_loc.end-1))\n",
    "\n",
    "    # check the tokens and strings match\n",
    "    lemma_a = var_loc_x[\"a_lemma\"].value\n",
    "    lemma_b = var_loc_x[\"b_lemma\"].value\n",
    "    recon_a = tokenizer.decode(batch.input_ids[0, a_toklocs[0]:a_toklocs[1]+1]).strip()\n",
    "    recon_b = tokenizer.decode(batch.input_ids[0, b_toklocs[0]:b_toklocs[1]+1]).strip()\n",
    "    ## assertion\n",
    "    assert lemma_a == recon_a\n",
    "    assert lemma_b == recon_b\n",
    "\n",
    "    last_hidden = out.hidden_states[-1]\n",
    "    vec_a = last_hidden[0, a_toklocs[0]:a_toklocs[1]+1].mean(0)\n",
    "    vec_b = last_hidden[0, b_toklocs[0]:b_toklocs[1]+1].mean(0)\n",
    "    logits = out.logits\n",
    "    data_ids = batch.input_ids.squeeze().to(\"cpu\").tolist()\n",
    "\n",
    "    ab_logits = logits[:, :, torch.tensor(symlist)]\n",
    "    ab_data_ids = [rev_symtable[x] for x in data_ids]\n",
    "    ab_input_ids = torch.tensor(ab_data_ids).view(1, -1).to(\"cuda\")\n",
    "\n",
    "    probs = torch.softmax(ab_logits, dim=1).squeeze().to(\"cpu\")\n",
    "    uniform_prob = (torch.ones(probs.shape[1]) / probs.shape[1]).to(\"cpu\")\n",
    "    next_token_probs = torch.concat([uniform_prob.unsqueeze(0), probs[:-1, :]], dim=0).cpu()     \n",
    "    tok_logits = ab_logits.gather(2, ab_input_ids[:, 1:].unsqueeze(2)).squeeze().to(\"cpu\")\n",
    "    tok_logits = torch.cat([torch.zeros(1,), tok_logits], dim=0)\n",
    "    logit_a = tok_logits[a_toklocs[0]:a_toklocs[1]+1].mean(0).item()\n",
    "    logit_b = tok_logits[b_toklocs[0]:b_toklocs[1]+1].mean(0).item()\n",
    "    tok_probs = next_token_probs.gather(1, ab_input_ids.transpose(1,0).cpu()).squeeze().to(\"cpu\")\n",
    "    prob_a = tok_probs[a_toklocs[0]:a_toklocs[1]+1].mean(0).item()\n",
    "    prob_b = tok_probs[b_toklocs[0]:b_toklocs[1]+1].mean(0).item()    \n",
    "\n",
    "    # do compression\n",
    "    compressor = AECompressorLLM()\n",
    "\n",
    "    # only compress the different part of emp/perm\n",
    "    ab_data_ids = ab_data_ids[b_toklocs[1]:]\n",
    "    next_token_probs = next_token_probs[b_toklocs[1]:]\n",
    "    try:\n",
    "        msg = compressor.compress(ab_data_ids, next_token_probs)\n",
    "        recon = compressor.decompress(msg, len(ab_data_ids), next_token_probs)\n",
    "        compress_bits = len(msg)\n",
    "        compress_ratio = compress_bits / (len(ab_data_ids) * symbol_bits)\n",
    "\n",
    "        assert all(a==b for a, b in zip(recon, ab_data_ids))\n",
    "    except Exception as ex:\n",
    "        compress_bits = float('nan')\n",
    "        compress_ratio = float('nan')\n",
    "        print(type(ex).__name__, str(ex))        \n",
    "        \n",
    "    # put vectors, logits, input_ids into inst_x\n",
    "    inst_x.update({\n",
    "        \"vec_a\": vec_a.cpu().numpy(),\n",
    "        \"vec_b\": vec_b.cpu().numpy(),\n",
    "        \"logit_a\": logit_a,\n",
    "        \"logit_b\": logit_b,\n",
    "        \"prob_a\": prob_a,\n",
    "        \"prob_b\": prob_b,\n",
    "        \"tok_logits\": tok_logits.numpy(),\n",
    "        \"tok_probs\": tok_probs.numpy(),        \n",
    "        \"compress_bits\": compress_bits,\n",
    "        \"compress_ratio\": compress_ratio,\n",
    "        \"input_ids\": batch.input_ids.cpu().numpy(),\n",
    "        \"ab_data_ids\": ab_data_ids,\n",
    "        \"n_ab_data_ids\": len(ab_data_ids),\n",
    "    })\n",
    "    proc_data.append(inst_x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5b2376",
   "metadata": {},
   "source": [
    "## Export artefacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e3f5094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artefact size (MB): 49\n"
     ]
    }
   ],
   "source": [
    "out_dir = Path(\"../data/minrep/\")\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "out_path = out_dir / f\"minrep-{model_name}-rev{rev_idx}.pkl\"\n",
    "fsize = out_path.write_bytes(pickle.dumps(proc_data))\n",
    "print(\"artefact size (MB):\", fsize//(1024**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35311a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "38892c258a249961395c0a0a7682683c65a89f99  ../data/minrep/minrep-pythia-6.9b.pkl\n"
     ]
    }
   ],
   "source": [
    "!sha1sum {str(out_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab27067",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
