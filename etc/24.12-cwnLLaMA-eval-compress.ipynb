{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d29ca1-095a-4615-b372-b8725424efb9",
   "metadata": {},
   "source": [
    "## Data dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e7bdb7b-e7e2-411f-ae40-7771a3a5f026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2e101d021306406001c3c4a6c3bf3679bc4149e9  ../data/cwn-prompt-500-perm-eval.csv\n",
      "3ef55f3f5ed97f86f8e3be0274e1721a01782f98  ../data/cwn-prompt-symtable.json\n"
     ]
    }
   ],
   "source": [
    "!sha1sum ../data/cwn-prompt-500-perm-eval.csv # 14.20\n",
    "!sha1sum ../data/cwn-prompt-symtable.json # 24.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc11f4c1-729a-47aa-9d2d-beae58da1c95",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbc43214",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "use_model = \"twLLaMA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6fd9cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twLlama\n"
     ]
    }
   ],
   "source": [
    "if use_model == \"cwnLLaMA\":\n",
    "  model_name = \"cwnLlama\"\n",
    "  repo_name = \"/mnt/md0/models/LoLLaMA/cwn-Taiwan-LLaMa/\"\n",
    "elif use_model == \"twLLaMA\":\n",
    "  model_name = \"twLlama\"\n",
    "  repo_name = \"yentinglin/Taiwan-LLaMa-v1.0\"\n",
    "else:\n",
    "  raise ValueError(\"Invalid model name\")\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5df5986-3cac-4936-ba13-75a215651065",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3445393f-a4a9-4981-a585-977b4201a78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"BNB_CUDA_VERSION\"] = \"115\"\n",
    "if \"../src/llm_compressor/src\" not in sys.path:\n",
    "  sys.path.append(\"../src/llm_compressor/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5df8c8f9-4a47-4978-bd81-0a07152e3daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seantyh/miniconda3/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:106: UserWarning: \n",
      "\n",
      "================================================================================\n",
      "WARNING: Manual override via BNB_CUDA_VERSION env variable detected!\n",
      "BNB_CUDA_VERSION=XXX can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "Loading CUDA version: BNB_CUDA_VERSION=115\n",
      "================================================================================\n",
      "\n",
      "\n",
      "  warn((f'\\n\\n{\"=\"*80}\\n'\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, GPTNeoXForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import list_repo_refs \n",
    "\n",
    "from llm_compressor import AECompressorLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c7f02c0-22c5-4d74-9bab-85ba50bd3ca8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/cwn-prompt-500-perm-eval.csv\")\n",
    "symtable = json.loads(Path(\"../data/cwn-prompt-symtable.json\").read_text())\n",
    "rev_symtable = {v: int(k) for k, v in symtable.items()}\n",
    "symlist = list(symtable.values())\n",
    "n_symbol = len(symlist)\n",
    "symbol_bits = np.log2(n_symbol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9168c03d-27b4-43b2-bbfa-1c063065252f",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc0a651",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65889625-b934-4072-93a8-4fe1f9936e88",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reformat the rev_list in reverse order\n",
    "if use_model == \"twLLaMA\":\n",
    "  cache_dir = Path(\"/home/seantyh/hdd/hf_cache/twLLaMA/main\")\n",
    "else:\n",
    "  cache_dir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60d6d7f4-619e-4121-96f3-6bfbe09a6096",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004615306854248047,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334c4181bd904a2390f8d963fe04a4b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seantyh/miniconda3/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/seantyh/miniconda3/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(repo_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  repo_name,\n",
    "  cache_dir=cache_dir,\n",
    "  load_in_8bit=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0d39a9a-827d-4e3f-9943-832f3ba25f66",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "VarLoc = namedtuple(\"VarLoc\", (\"name\", \"value\", \"start\", \"end\"))\n",
    "\n",
    "def make_locmap(inst):\n",
    "  return {\n",
    "    x[0]: VarLoc(*x)\n",
    "    for x in inst[\"var_loc\"]\n",
    "  }\n",
    "\n",
    "def find_token_locs(batch, char_locs):\n",
    "  locs = [\n",
    "    batch.char_to_token(0, x)\n",
    "    for x in char_locs\n",
    "  ]\n",
    "  if locs[-1] is None:\n",
    "    locs[-1] = len(batch.input_ids[0])-1\n",
    "  return locs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1db0a0-2253-4ab9-ae88-71c25c6774fd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752a149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_data = []\n",
    "for _, row in tqdm(data.iterrows(), total=len(data)):\n",
    "    prompt = row.prompt\n",
    "    batch = tokenizer(prompt, \n",
    "                    truncation=True,\n",
    "                    max_length=1000,\n",
    "                    return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        out = model(**batch, output_hidden_states=True)\n",
    "\n",
    "\n",
    "    b_start, b_end = [int(x) for x in row.resp_loc[1:-1].split(\",\")]\n",
    "    b_toklocs = find_token_locs(batch, (b_start, b_end))\n",
    "\n",
    "    last_hidden = out.hidden_states[-1]\n",
    "    vec_b = last_hidden[0, b_toklocs[0]:b_toklocs[1]].mean(0)\n",
    "    logits = out.logits\n",
    "    data_ids = batch.input_ids.squeeze().to(\"cpu\").tolist()\n",
    "\n",
    "    ab_logits = logits[:, :, torch.tensor(symlist)]\n",
    "    ab_data_ids = [rev_symtable[x] for x in data_ids]\n",
    "    ab_input_ids = torch.tensor(ab_data_ids).view(1, -1).to(\"cuda\")\n",
    "\n",
    "    probs = torch.softmax(ab_logits, dim=1).squeeze().to(\"cpu\")\n",
    "    uniform_prob = (torch.ones(probs.shape[1]) / probs.shape[1]).to(\"cpu\")\n",
    "    next_token_probs = torch.concat([uniform_prob.unsqueeze(0), probs[:-1, :]], dim=0).cpu()     \n",
    "    tok_logits = ab_logits.gather(2, ab_input_ids[:, 1:].unsqueeze(2)).squeeze().to(\"cpu\")\n",
    "    tok_logits = torch.cat([torch.zeros(1,), tok_logits], dim=0)\n",
    "    logit_b = tok_logits[b_toklocs[0]:b_toklocs[1]].mean(0).item()\n",
    "    tok_probs = next_token_probs.gather(1, ab_input_ids.transpose(1,0).cpu()).squeeze().to(\"cpu\")\n",
    "    prob_b = tok_probs[b_toklocs[0]:b_toklocs[1]].mean(0).item()    \n",
    "\n",
    "    # do compression\n",
    "    compressor = AECompressorLLM()\n",
    "\n",
    "    # only compress the different part of emp/perm\n",
    "    ab_data_ids = ab_data_ids[b_toklocs[0]:]\n",
    "    next_token_probs = next_token_probs[b_toklocs[0]:]\n",
    "    try:\n",
    "        msg = compressor.compress(ab_data_ids, next_token_probs)\n",
    "        recon = compressor.decompress(msg, len(ab_data_ids), next_token_probs)\n",
    "        compress_bits = len(msg)\n",
    "        compress_ratio = compress_bits / (len(ab_data_ids) * symbol_bits)\n",
    "\n",
    "        assert all(a==b for a, b in zip(recon, ab_data_ids))\n",
    "    except Exception as ex:\n",
    "        compress_bits = float('nan')\n",
    "        compress_ratio = float('nan')\n",
    "        print(type(ex).__name__, str(ex))        \n",
    "        \n",
    "    # put vectors, logits, input_ids into inst_x\n",
    "    inst_x = {**row.to_dict(), \n",
    "        \"logit_b\": logit_b,\n",
    "        \"prob_b\": prob_b,\n",
    "        \"tok_logits\": tok_logits.numpy(),\n",
    "        \"tok_probs\": tok_probs.numpy(),        \n",
    "        \"compress_bits\": compress_bits,\n",
    "        \"compress_ratio\": compress_ratio,\n",
    "        \"input_ids\": batch.input_ids.cpu().numpy(),\n",
    "        \"ab_data_ids\": ab_data_ids,\n",
    "        \"n_ab_data_ids\": len(ab_data_ids),\n",
    "    }\n",
    "    proc_data.append(inst_x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2c2b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.715602938969356, 0.7959351808503101)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_cr = np.array([x[\"compress_ratio\"] for x in proc_data if x[\"prompt_type\"] == \"emp\"])\n",
    "perm_cr = np.array([x[\"compress_ratio\"] for x in proc_data if x[\"prompt_type\"] == \"perm\"])\n",
    "emp_cr.mean(), perm_cr.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5b2376",
   "metadata": {},
   "source": [
    "## Export artefacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3f5094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artefact size (MB): 49\n"
     ]
    }
   ],
   "source": [
    "out_dir = Path(\"../data/minrep/\")\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "out_path = out_dir / f\"minrep-{model_name}-cwn-eval.pkl\"\n",
    "fsize = out_path.write_bytes(pickle.dumps(proc_data))\n",
    "print(\"artefact size (MB):\", fsize//(1024**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35311a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "38892c258a249961395c0a0a7682683c65a89f99  ../data/minrep/minrep-pythia-6.9b.pkl\n"
     ]
    }
   ],
   "source": [
    "!sha1sum {str(out_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab27067",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
